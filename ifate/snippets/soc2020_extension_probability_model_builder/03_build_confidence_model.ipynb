{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6963c4",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook Summary\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- Build a confidence model that that tells us how confident we are that any match is correct.\n",
    "   - It predicts probablity that Standard-SOC is a match.\n",
    "- Output data with probability for all matches for use.\n",
    "\n",
    "\n",
    "## What is in this notebook\n",
    "\n",
    "- Read in training data from `02_build_and_plot_features.ipynb`\n",
    "   - The two parts of the version number referes to which output to read.\n",
    "- Build a Random Forest model that predicts at a row level if SOC matches\n",
    "- Check peformance\n",
    "   - Calibration\n",
    "   - Performance (precision vs. recall)\n",
    "- Make predictions for all matches (using cross-fold to avoid bias)\n",
    "      \n",
    "## Output/Results\n",
    "\n",
    "- File in format `f'nsfg_data/df_output_modelling_{run_version}.csv'`\n",
    " - This output data which will be used in next stage to create a product.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8870b97",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a79387",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_version = 'v4.2.2' \n",
    "# .2 include core and options as feature\n",
    "# .1 Just model B, Include standards that don't have label in output\n",
    "\n",
    "# Get everything before 2nd .\n",
    "data_version = '.'.join(run_version.split('.')[0:2])\n",
    "\n",
    "label = 'autoassign_in_top_3' # What to use as target variable\n",
    "\n",
    "do_tuning = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93770802",
   "metadata": {},
   "source": [
    "## Regular Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393df068",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports \n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac1603",
   "metadata": {},
   "source": [
    "## My Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Path of where you have imported my functions\n",
    "current_path = os.getcwd()\n",
    "functions_path = Path('..', 'Functions')\n",
    "sys.path.append(str(functions_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pandas functions\n",
    "import laurie_pandas_functions as pd_funcs\n",
    "from laurie_pandas_functions import display_full\n",
    "\n",
    "## Matplotlib funcs\n",
    "import laurie_plotting_functions as plot_funcs\n",
    "from laurie_plotting_functions import get_ax, force_ax_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a76206",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful when developing your functions\n",
    "from importlib import reload  \n",
    "reload(pd_funcs)\n",
    "reload(plot_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ls_on_path(path):\n",
    "    \"\"\"\n",
    "    Run ls on a path in jupyter and display to notebook\n",
    "    Can't be imported as uses cell magic\n",
    "    Args: path (pathlib.WindowsPath): path created by pathlib\n",
    "    \"\"\"\n",
    "    userhome = os.path.expanduser(\"~\")\n",
    "    reformatted_path = ('\\\"' + str(path).replace('\\\\\\\\', '\\\"/\\\"') + '\\\"').replace('\\\"~\\\"','~').replace('~', userhome)\n",
    "    print(f'$ ls {path}')\n",
    "    !ls {reformatted_path}\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e28d95b",
   "metadata": {},
   "source": [
    "## Pandas/Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Colours\n",
    "blue =  '#79a5f7'\n",
    "red  =  '#ff9696'\n",
    "green=  '#9ebd9e'\n",
    "sns_colours = sns.color_palette()\n",
    "\n",
    "### Plot Size (supersize_me)\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'axes.labelsize': 'large',\n",
    "          'axes.titlesize':'large',\n",
    "          'xtick.labelsize':'large',\n",
    "          'ytick.labelsize':'large',\n",
    "          'figure.titlesize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8a9df",
   "metadata": {},
   "source": [
    "## ML Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed991d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caefc1d4",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_raw = pd.read_csv(f'nsfg_data/df_train_w_features__{data_version}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74098b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_raw['standard_code'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b02133",
   "metadata": {},
   "source": [
    "## Fill NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb92cad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_nulls = df_training_raw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls[count_nulls > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7644e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na_minus_1 = [\n",
    "    'soc_job_matches_standard_title',\n",
    "    'standard_typical_job_title',\n",
    "    'soc_job_matches_typical_job',\n",
    "    'score_soc_job_match_typical_job',\n",
    "    'score_soc_job_match_typical_job_minus_alt_1',\n",
    "    'score_soc_job_match_typical_job_minus_alt_2',        \n",
    "]\n",
    "\n",
    "df_training_no_nulls = df_training_raw.copy()\n",
    "for col in fill_na_minus_1:\n",
    "    df_training_no_nulls[col] = df_training_no_nulls[col].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef6441",
   "metadata": {},
   "source": [
    "## Remove Those with out labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81541b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_no_nulls['n_labels_for_ref_no'] = (\n",
    "    df_training_no_nulls\n",
    "    .groupby('ref_no')\n",
    "    [label]\n",
    "    .transform(sum)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_rm_no_labels = df_training_no_nulls.loc[lambda df: df['n_labels_for_ref_no'] > 0]\n",
    "df_training_no_labels = df_training_no_nulls.loc[lambda df: df['n_labels_for_ref_no'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262fb35",
   "metadata": {},
   "source": [
    "## Get Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df_training_rm_no_labels\n",
    "df_training_top_ranks = df_training.loc[lambda df: df['score_rank'] <= 50]\n",
    "df_training_not_top_ranks = df_training.loc[lambda df: df['score_rank'] > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290951ca",
   "metadata": {},
   "source": [
    "# Modelling Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07ed89",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilites(model, X, class_index=1):\n",
    "    \"\"\"\n",
    "    Get probabilities that model thinks item is in class_index for features X\n",
    "    \n",
    "    Args:\n",
    "        model: scikit-learn classifier\n",
    "        X (pd.df): DF containing features for model\n",
    "        class_index (int): Which class are we predicting, default = 1\n",
    "        \n",
    "    Returns:\n",
    "        y (np.array): List of probabilites that model thinks item is in class_index\n",
    "    \"\"\"\n",
    "    return model.predict_proba(X)[:,class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f56d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_and_test(data, x, ax, metric='log_loss', x_seperators=None):\n",
    "    \"\"\"\n",
    "    Plot line plots of balanced_accuracy for test and train\n",
    "    \"\"\"\n",
    "    for test_set in ['train', 'test']:\n",
    "        y = f'{metric}_{test_set}'\n",
    "        sns.lineplot(data=data, x=x, y=y, ax=ax, label=test_set.title())\n",
    "    \n",
    "    ax.set(ylabel = 'Balanced Accuracy')\n",
    "    force_ax_grid(ax, x_seperators=x_seperators)\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1983d",
   "metadata": {},
   "source": [
    "## Set QA Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_NO_QA = 0.7  ## What % will we accept doing no QA\n",
    "SCORE_LOW_QA = 0.5  ## What % will we accept a low-level QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b995b72",
   "metadata": {},
   "source": [
    "## Print Out Scenario Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out_scenario_two_cuts(df_scenario, prob_col, cut_off_no_qa, cut_off_low_level_qa, data_label):\n",
    "    \n",
    "    n_scenario = df_scenario.shape[0]\n",
    "\n",
    "    ## No QA\n",
    "    df_no_qa = df_scenario.loc[lambda df: df[prob_col] > cut_off_no_qa]  \n",
    "    \n",
    "    \n",
    "    df_no_qa_stats = df_no_qa[label].agg(['count','sum','mean'])\n",
    "    \n",
    "    n_no_qa_scenario = df_no_qa_stats['count']\n",
    "    pct_no_qa_scenario = n_no_qa_scenario * 100.0 / n_scenario\n",
    "    n_no_qa_correct_scenario = df_no_qa_stats['sum']\n",
    "    accuracy_no_qa_scenario = df_no_qa_stats['mean'] * 100.0\n",
    "    \n",
    "    ## Low-level QA\n",
    "    df_low_level_qa = (\n",
    "        df_scenario\n",
    "        .loc[lambda df: df[prob_col] > cut_off_low_level_qa]    \n",
    "        .loc[lambda df: df[prob_col] <= cut_off_no_qa]    \n",
    "    )\n",
    "    \n",
    "    df_low_level_qa_stats = df_low_level_qa[label].agg(['count','sum','mean'])\n",
    "    \n",
    "    n_low_level_qa_scenario = df_low_level_qa_stats['count']\n",
    "    pct_low_level_qa_scenario = n_low_level_qa_scenario * 100.0 / n_scenario\n",
    "    n_low_level_qa_correct_scenario = df_low_level_qa_stats['sum']\n",
    "    accuracy_low_level_qa_scenario = df_low_level_qa_stats['mean'] * 100.0\n",
    "\n",
    "    ## Full QA\n",
    "    df_full_qa = (\n",
    "        df_scenario\n",
    "        .loc[lambda df: df[prob_col] <= cut_off_low_level_qa]    \n",
    "    )\n",
    "    \n",
    "    df_full_qa_stats = df_full_qa[label].agg(['count','sum','mean'])\n",
    "    \n",
    "    n_full_qa_scenario = df_full_qa_stats['count']\n",
    "    pct_full_qa_scenario = n_full_qa_scenario * 100.0 / n_scenario\n",
    "    n_full_qa_correct_scenario = df_full_qa_stats['sum']\n",
    "    accuracy_full_qa_scenario = df_full_qa_stats['mean'] * 100.0\n",
    "    \n",
    "    print(f'''\n",
    "    In {data_label},\n",
    "    which is {n_scenario} standards.\n",
    "    \n",
    "    If we were not to QA anything with a probability score > {cut_off_no_qa * 100.0:,.0f}%\n",
    "    And only apply low-level QA to anything with a probability score > {cut_off_low_level_qa * 100.0:,.0f}%\n",
    "    \n",
    "    Then we would not do QA for\n",
    "    - {n_no_qa_scenario:,.0f} Standards ({pct_no_qa_scenario:.1f}%)\n",
    "    - The accuracy would be {accuracy_no_qa_scenario:.1f}%\n",
    "    \n",
    "    Only do low-level QA for \n",
    "    - {n_low_level_qa_scenario:,.0f} Standards ({pct_low_level_qa_scenario:.1f}%)\n",
    "    - The accuracy would be {accuracy_low_level_qa_scenario:.1f}%\n",
    "    \n",
    "    And finally do QA for \n",
    "    - {n_full_qa_scenario:,.0f} Standards ({pct_full_qa_scenario:.1f}%)\n",
    "    - The accuracy would be {accuracy_full_qa_scenario:.1f}%\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out_scenarios_two_cuts(df_predictions_model, prob_col, cut_off_no_qa, cut_off_low_level_qa):\n",
    "\n",
    "    for i, (scenario_desc, scenario_filter) in enumerate(dict_scenario_desc_and_filter.items()):\n",
    "        \n",
    "        scenario_number = i + 1\n",
    "        if scenario_filter:\n",
    "            df_scenario = df_predictions_model.loc[scenario_filter]\n",
    "        else:\n",
    "            df_scenario = df_predictions_model\n",
    "    \n",
    "        print('\\n---------------------------')\n",
    "        print(f'Scenario {scenario_number}:')\n",
    "        print_out_scenario_two_cuts(df_scenario, prob_col, cut_off_no_qa, cut_off_low_level_qa, scenario_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1eb209",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c544138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_calibration(df_predictions, rank_col, prob_col, model_name='Model', plot_top_rank=False):\n",
    "    \n",
    "    df_predictions['model_output_prob_grouped'] = (df_predictions[prob_col]).round(1)\n",
    "    \n",
    "    df_calibration = (\n",
    "        df_predictions\n",
    "       .groupby('model_output_prob_grouped')\n",
    "        .agg(**{\n",
    "            'count': (prob_col, 'count'),\n",
    "            prob_col: ( prob_col, 'mean'),\n",
    "            label: (label, 'mean'),\n",
    "        }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    df_top_predictions = df_predictions.loc[lambda df: df[rank_col]==1]\n",
    "    \n",
    "    df_calibration_top = (\n",
    "        df_top_predictions\n",
    "       .groupby('model_output_prob_grouped')\n",
    "        .agg(\n",
    "            {prob_col:'mean',\n",
    "             label:'mean'\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    df_calibration = (\n",
    "        df_calibration\n",
    "        .merge(\n",
    "            df_calibration_top,\n",
    "            how='outer', \n",
    "            on='model_output_prob_grouped',\n",
    "            suffixes=('', '__top_rank')\n",
    "        )\n",
    "        \n",
    "    )\n",
    "    \n",
    "    display(\n",
    "        df_calibration\n",
    "        .set_index('model_output_prob_grouped')\n",
    "        .style\n",
    "        .format('{:,.2f}')\n",
    "        .set_table_styles(pd_funcs.get_lauries_table_styles())\n",
    "    )\n",
    "    \n",
    "    ax = get_ax(width=7, height=7)\n",
    "    \n",
    "    (\n",
    "        df_calibration \n",
    "        .plot(x = prob_col, y = label, ax = ax, marker = 'o', linestyle = ':', label = 'Model Performance')\n",
    "    )\n",
    "    \n",
    "    if plot_top_rank:\n",
    "    \n",
    "        (\n",
    "            df_calibration \n",
    "            .plot(x = 'model_output_prob__top_rank', y = 'soc2020_autoassign_correct__top_rank', ax = ax, marker = 'o', linestyle = ':', label = 'Model Performance (Top Rank)')\n",
    "        )\n",
    "    \n",
    "    \n",
    "    (\n",
    "        df_calibration \n",
    "        .assign(model_output_prob2 = lambda df: df[prob_col])\n",
    "        .plot(x = prob_col, y = 'model_output_prob2', ax = ax, linestyle = '--', color = 'dimgrey', zorder = 0, label = 'Ideal')\n",
    "    )\n",
    "    \n",
    "    \n",
    "    force_ax_grid(ax, x_seperators=0.1, y_seperators=0.1)\n",
    "    ax.set(xlabel = 'Probability from Model',\n",
    "           ylabel = 'True Probability',\n",
    "           title = 'Calibration Plot - Probability that Auto-Assign is Correct'\n",
    "          )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ac836",
   "metadata": {},
   "source": [
    "## Plot Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d843d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_scenario_desc_and_filter = {\n",
    "    f'Entire data-set - Cross-folding': None, ## Filter for all,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacb8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probs(df_predictions, prob_col, rank_col):\n",
    "    \n",
    "    dict_rank_desc_and_filter = {\n",
    "        'All Ranks': lambda df: df[rank_col] >= 0,\n",
    "        'Top 10': lambda df: df[rank_col] <= 5,\n",
    "        'Top Rank': lambda df: df[rank_col] <= 1,\n",
    "    }\n",
    "\n",
    "    for rank_desc, rank_filter in dict_rank_desc_and_filter.items():\n",
    "        for scenario_desc, scenario_filter in dict_scenario_desc_and_filter.items():\n",
    "    \n",
    "            print(scenario_desc, rank_desc)\n",
    "            \n",
    "            data = df_predictions.loc[rank_filter]\n",
    "            if scenario_filter: data = data[scenario_filter]\n",
    "                \n",
    "            ax = get_ax()\n",
    "            \n",
    "            bins = np.linspace(0, 1, 21)\n",
    "            \n",
    "            sns.histplot(\n",
    "                data=data,\n",
    "                x=prob_col, hue=label,\n",
    "                ax=ax, stat='percent', common_norm=False, bins=bins,\n",
    "            )\n",
    "            \n",
    "            ax.set(title = f'{scenario_desc} - {rank_desc}'.replace('the', 'The'))\n",
    "            force_ax_grid(ax)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fc6f7",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics_df(data, label, rank_col, prob_col=None):\n",
    "\n",
    "    dict_performance_metrics = {}\n",
    "    \n",
    "    \n",
    "    data['total_ranked'] = data.loc[lambda df: df[rank_col]==1][label].sum()\n",
    "    \n",
    "    dict_ranks_and_filters = {\n",
    "        'Top Rank': data[rank_col] == 1,\n",
    "        'Top 5': data[rank_col] <= 5,\n",
    "        'Top 10': data[rank_col] <= 10,\n",
    "        'Top 50': data[rank_col] <= 50,\n",
    "    }\n",
    "    \n",
    "    for rank_str, rank_filter in dict_ranks_and_filters.items():\n",
    "        \n",
    "        dict_performance_metrics[f'Recall of {rank_str}'] = (\n",
    "            (\n",
    "                data\n",
    "                .loc[rank_filter]\n",
    "                .groupby('ref_no')\n",
    "                [label]\n",
    "                .sum()\n",
    "            ) / (\n",
    "                data\n",
    "                .groupby('ref_no')\n",
    "                [label]\n",
    "                .sum()\n",
    "            )\n",
    "        ).mean()\n",
    "        \n",
    "\n",
    "        dict_performance_metrics[f'Precision of {rank_str}'] = (\n",
    "            (\n",
    "                data\n",
    "                .loc[rank_filter]\n",
    "                .groupby('ref_no')\n",
    "                [label]\n",
    "                .mean()\n",
    "            ) \n",
    "        ).mean()\n",
    "\n",
    "        \n",
    "    \n",
    "    if prob_col:\n",
    "        \n",
    "        dict_performance_metrics['logloss_score'] = metrics.log_loss(data[label], data[prob_col])\n",
    "        \n",
    "        prob_str = '> 50% Prob'\n",
    "        prob_filter =  data[prob_col] >= 0.5\n",
    "        \n",
    "        dict_performance_metrics[f'Recall of {prob_str}'] = (\n",
    "            (\n",
    "                data\n",
    "                .loc[prob_filter]\n",
    "                .groupby('ref_no')\n",
    "                [label]\n",
    "                .sum()\n",
    "            ) / (\n",
    "                data\n",
    "                .groupby('ref_no')\n",
    "                [label]\n",
    "                .sum()\n",
    "            )\n",
    "        ).mean()\n",
    "        \n",
    "\n",
    "        dict_performance_metrics[f'Precision of {prob_str}'] = (\n",
    "            (\n",
    "                data\n",
    "                .loc[prob_filter]\n",
    "                .groupby('ref_no')\n",
    "                [label]\n",
    "                .mean()\n",
    "            ) \n",
    "        ).mean()\n",
    "\n",
    "        \n",
    "    return dict_performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c177bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance_metrics_df(dict_perf_metrics):\n",
    "    print('*******************')\n",
    "    for metric_name, metric in dict_perf_metrics.items():\n",
    "        if metric_name.startswith('Recall') or metric_name.startswith('Precision'):\n",
    "            metric = f'{metric*100:.1f}%'\n",
    "        else:\n",
    "            metric = f'{metric:.4f}'\n",
    "        \n",
    "        output = f'{metric_name:25} : ' + metric\n",
    "        print(output)\n",
    "    print('*******************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11821ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## No model, just use jobtitle_rank\n",
    "dict_no_model_perf = get_performance_metrics_df(df_training, label=label, rank_col='score_rank')\n",
    "display_performance_metrics_df(dict_no_model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc663f",
   "metadata": {},
   "source": [
    "# B - RF Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f00d9",
   "metadata": {},
   "source": [
    "## B - Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7729cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eaf155",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_model_feature_cols_model_b = [\n",
    "    'score_soc_job_match_standard_title',\n",
    "    'score_soc_job_match_standard_title_minus_alt_1',\n",
    "    'score_soc_job_match_standard_title_minus_alt_2',\n",
    "    'score_soc_job_match_typical_job',\n",
    "    'score_soc_job_match_typical_job_minus_alt_1',\n",
    "    'score_soc_job_match_typical_job_minus_alt_2',\n",
    "    'score_overview',\n",
    "    'score_overview_minus_alt_1',\n",
    "    'score_overview_minus_alt_2',\n",
    "    'soc_2020_matches_previous_assignment',\n",
    "    'soc_2020_matches_previous_assignment_minus_alt_1',\n",
    "    'soc_2020_matches_previous_assignment_minus_alt_2',\n",
    "    'soc_2020_ext_is_nec',\n",
    "    'absolute_relative_distance_between_level_and_soc_major_group',\n",
    "    'absolute_relative_distance_between_level_and_soc_major_group_minus_alt_1',\n",
    "    'absolute_relative_distance_between_level_and_soc_major_group_minus_alt_2',\n",
    "    'is_core_and_options'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_top_ranks = df_training.loc[lambda df: df['score_rank'] <= 50]\n",
    "X_model_b = df_training_top_ranks[list_model_feature_cols_model_b]\n",
    "y_model_b = df_training_top_ranks[label]\n",
    "groups = df_training_top_ranks['ref_no']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8619e834",
   "metadata": {},
   "source": [
    "## B - Scan Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70236554",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if do_tuning:\n",
    "    ### Lists to store data. I will create a pandas df from this.\n",
    "    dict_rf_scan_min_split = {\n",
    "        'n_estimators': [],\n",
    "        'max_depth': [],\n",
    "        'log_loss_train': [],\n",
    "        'log_loss_test': [],\n",
    "        'min_samples_split': [],\n",
    "        'model': [],\n",
    "    }\n",
    "    \n",
    "    X = X_model_b\n",
    "    y = y_model_b\n",
    "    \n",
    "    ## Ranges\n",
    "    n_estimators = 100\n",
    "    max_depths = [5, 8, 10, 12, 15, 20]\n",
    "    min_samples_split_choices = [50, 75, 100, 150, 200] \n",
    "    \n",
    "    \n",
    "    for max_depth in max_depths:\n",
    "        for min_samples_split in min_samples_split_choices:\n",
    "            \n",
    "            timestamp = datetime.datetime.today().strftime('%y/%m/%d %H:%M:%S')\n",
    "            print(f'{timestamp} - {max_depth:6}, {n_estimators:6}, {min_samples_split:6,.0f}')\n",
    "        \n",
    "            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "            model.fit(X, y)\n",
    "            \n",
    "            probs_train = get_probabilites(model, X)\n",
    "            log_loss_train = metrics.log_loss(y, probs_train)\n",
    "            \n",
    "            gkf = model_selection.GroupKFold(3)\n",
    "            probs_test = model_selection.cross_val_predict(model, X, y, cv=gkf, groups=groups, method='predict_proba')\n",
    "            log_loss_test = metrics.log_loss(y, probs_test)\n",
    "                \n",
    "            dict_rf_scan_min_split['n_estimators'].append(n_estimators)\n",
    "            dict_rf_scan_min_split['max_depth'].append(max_depth)\n",
    "            dict_rf_scan_min_split['log_loss_train'].append(log_loss_train)\n",
    "            dict_rf_scan_min_split['log_loss_test'].append(log_loss_test)\n",
    "            dict_rf_scan_min_split['min_samples_split'].append(min_samples_split)\n",
    "            dict_rf_scan_min_split['model'].append(model)\n",
    "            \n",
    "    ## C - Store results in a df\n",
    "    df_scan_model_b = pd.DataFrame(dict_rf_scan_min_split)\n",
    "    df_scan_model_b['min_samples_split_str'] = df_scan_model_b['min_samples_split'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073007b4",
   "metadata": {},
   "source": [
    "## B - Pick Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b45601",
   "metadata": {},
   "source": [
    "### Just see test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_tuning:\n",
    "    ax = get_ax()\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data = df_scan_model_b,\n",
    "        x = 'min_samples_split_str', y = 'log_loss_test', ax = ax,\n",
    "        hue = 'max_depth', marker = 'o'\n",
    "    )\n",
    "    force_ax_grid(ax)\n",
    "    ax.set(ylabel = 'LogLoss', xlabel = 'Min Samples Split')\n",
    "    \n",
    "    ax = get_ax()\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data = df_scan_model_b,\n",
    "        x = 'max_depth', y = 'log_loss_test', ax = ax,\n",
    "        hue = 'min_samples_split', marker = 'o'\n",
    "    )\n",
    "    force_ax_grid(ax, x_seperators=5)\n",
    "    ax.set(ylabel = 'LogLoss', xlabel = 'Max Depth')\n",
    "    \n",
    "    df_scan_model_b.sort_values('log_loss_test').head(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd9243",
   "metadata": {},
   "source": [
    "### See Test vs Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82e09b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if do_tuning:\n",
    "    ax = get_ax(nrows=1, width=10, height=4)\n",
    "    data = df_scan_model_b.loc[lambda df: df['max_depth']==10]\n",
    "    plot_validation_and_test(data, x='min_samples_split_str', ax=ax)\n",
    "    ax.set(ylabel = 'Log Loss', xlabel = 'Min Size for Split')\n",
    "    \n",
    "    ax = get_ax(nrows=1, width=10, height=4)\n",
    "    data = df_scan_model_b.loc[lambda df: df['min_samples_split']==100]\n",
    "    plot_validation_and_test(data, x='max_depth', ax=ax, x_seperators=5)\n",
    "    ax.set(ylabel = 'Log Loss', xlabel = 'Max Depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d173ac5f",
   "metadata": {},
   "source": [
    "## B - Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_model_b = 100\n",
    "min_samples_split_choices_model_b = 50\n",
    "depth_model_b = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = RandomForestClassifier(n_estimators =n_estimators_model_b, max_depth= depth_model_b, min_samples_split=min_samples_split_choices_model_b)\n",
    "model_b.fit(X_model_b, y_model_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217fba3",
   "metadata": {},
   "source": [
    "## B - Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe05d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_train = get_probabilites(model_b, X_model_b)\n",
    "log_loss_train = metrics.log_loss(y_model_b, probs_train)\n",
    "print(log_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e996a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = model_selection.GroupKFold(5)\n",
    "probs_test = model_selection.cross_val_predict(model_b, X_model_b, y_model_b, cv=gkf, groups=groups, method='predict_proba')[:,1]\n",
    "log_loss_test = metrics.log_loss(y_model_b, probs_test)\n",
    "\n",
    "print(f'''\n",
    "mean_train_score_model_b      = {log_loss_train:,.4f}\n",
    "mean_test_score_model_b       = {log_loss_test:,.4f}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf8ca8",
   "metadata": {},
   "source": [
    "## B - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_top_ranks_tmp = df_training.loc[lambda df: df['score_rank'] <= 50]\n",
    "df_training_not_top_ranks_tmp = df_training.loc[lambda df: df['score_rank'] > 50]\n",
    "\n",
    "df_training_top_ranks_tmp['model_b_output_prob'] = probs_test\n",
    "\n",
    "df_training_not_top_ranks_tmp['model_b_output_prob'] = get_probabilites(\n",
    "    model_b,\n",
    "    df_training_not_top_ranks_tmp[list_model_feature_cols_model_b]\n",
    ")\n",
    "\n",
    "df_training = pd.concat([\n",
    "    df_training_top_ranks_tmp,\n",
    "    df_training_not_top_ranks_tmp,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb426f",
   "metadata": {},
   "source": [
    "## B - Features Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "importances = model_b.feature_importances_\n",
    "std = np.std([\n",
    "    tree.feature_importances_ for tree in model_b.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: \"\n",
    "      f\"{elapsed_time:.3f} seconds\")\n",
    "\n",
    "dict_forest_importances = {\n",
    "    'feature': list_model_feature_cols_model_b,\n",
    "    'importance': importances,\n",
    "    'std': std\n",
    "}\n",
    "df_forest_importances = pd.DataFrame(dict_forest_importances)\n",
    "\n",
    "\n",
    "ax = get_ax(width=4)\n",
    "\n",
    "(\n",
    "    df_forest_importances\n",
    "    .sort_values('importance')\n",
    "    .plot(\n",
    "        kind='barh',  ax=ax,\n",
    "        x='feature', y= 'importance', xerr='std'\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set(title=\"Feature importances using MDI\", xlabel=\"Mean decrease in impurity\",ylabel='')\n",
    "force_ax_grid(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56c3e2",
   "metadata": {},
   "source": [
    "## B - Rank and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66657c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = (\n",
    "    df_training\n",
    "    .assign(\n",
    "        rank_model_b=lambda df: df.groupby('ref_no')['model_b_output_prob'].rank('first', ascending=False).astype(int),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_top_predictions_model_b = df_training.loc[lambda df: df['rank_model_b'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_performance_metrics_df(dict_no_model_perf)\n",
    "dict_model_b = get_performance_metrics_df(df_training, prob_col='model_b_output_prob', label=label, rank_col='rank_model_b')\n",
    "display_performance_metrics_df(dict_model_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064af9c2",
   "metadata": {},
   "source": [
    "## B - Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e772c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_calibration(df_training, prob_col='model_b_output_prob', rank_col='rank_model_b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b1ed2",
   "metadata": {},
   "source": [
    "## B - Plot Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157515d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_probs(df_training, prob_col='model_b_output_prob', rank_col='rank_model_b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdea5bf",
   "metadata": {},
   "source": [
    "## B - Print Out Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b541929",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_out_scenarios_two_cuts(df_top_predictions_model_b, 'model_b_output_prob', SCORE_NO_QA, SCORE_LOW_QA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ce5a0",
   "metadata": {},
   "source": [
    "# Summary of perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e9b30",
   "metadata": {},
   "source": [
    "## Print Outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No Model')\n",
    "display_performance_metrics_df(dict_no_model_perf)\n",
    "print('Model B')\n",
    "display_performance_metrics_df(dict_model_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0af07f",
   "metadata": {},
   "source": [
    "## Plot Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418aa392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recall(data, dict_rank_models, max_match_rank=1, xrange=50):\n",
    "\n",
    "    n_models = len(dict_rank_models)\n",
    "    bins = np.linspace(1, xrange, (xrange + 1))\n",
    "\n",
    "    ax = get_ax()\n",
    "    data_matches = data.loc[lambda df: df['match_rank'] <= max_match_rank]\n",
    "\n",
    "    for i, (model_name, model_rank_col) in enumerate(dict_rank_models.items()):\n",
    "       \n",
    "        sns.histplot(\n",
    "            data_matches,\n",
    "            x=model_rank_col,\n",
    "            bins=bins,\n",
    "            label=model_name,\n",
    "            ax=ax,\n",
    "            cumulative=True,\n",
    "            stat='percent',\n",
    "            fill=False,\n",
    "            element='step',\n",
    "            color=sns_colours[i]\n",
    "        )\n",
    "\n",
    "    force_ax_grid(ax)\n",
    "    ax.legend()\n",
    "    ax.set(title='What % ')\n",
    "    \n",
    "    return ax\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rank_models = {\n",
    "    'Baseline': 'score_rank',\n",
    "    'Model B': 'rank_model_b',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_recall(df_training, dict_rank_models, max_match_rank=1, xrange=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ddf113",
   "metadata": {},
   "source": [
    "## Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52571371",
   "metadata": {},
   "source": [
    "### PR Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_precision_recall_curve_to_ax(data, y_col, prob_col, model_name, ax):\n",
    "\n",
    "    probs = data[prob_col]\n",
    "    y = data[y_col]\n",
    "    \n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "    \n",
    "    thresholds = list(thresholds)\n",
    "    thresholds.append(1)\n",
    "    \n",
    "    pr_display = metrics.PrecisionRecallDisplay(\n",
    "        precision=precision, recall=recall,\n",
    "        estimator_name=model_name\n",
    "    )\n",
    "    \n",
    "    pr_display.plot(ax)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'thresholds': thresholds,\n",
    "    })\n",
    "        \n",
    "    return ax, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c874411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(list_values, find_value):\n",
    "    \n",
    "    closest = min(list_values, key=lambda x: abs(x-find_value))\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_precision_recall_annotation(ax, df_pr, threshold_to_annotate):\n",
    "    \n",
    "    threshold_annotate = get_closest(df_pr['thresholds'], threshold_to_annotate)\n",
    "    filter_annotate = df_pr['thresholds'] == threshold_annotate\n",
    "    precision_annotate = df_pr.loc[filter_annotate]['precision'].mean()\n",
    "    recall_annotate = df_pr.loc[filter_annotate]['recall'].mean()\n",
    "\n",
    "    annotation = '\\n'.join([\n",
    "        f'P > {threshold_annotate*100:,.0f}%',\n",
    "        f'Pre. = {precision_annotate*100:,.0f}%',\n",
    "        f'Rec. = {recall_annotate*100:,.0f}%',\n",
    "    ])\n",
    "    \n",
    "    ax.scatter(x=[recall_annotate], y=[precision_annotate], color='black', zorder=3)\n",
    "    ax.annotate(\n",
    "        annotation,\n",
    "        (recall_annotate, precision_annotate),\n",
    "        textcoords='offset points',\n",
    "        xytext=(10, 10),\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1209c",
   "metadata": {},
   "source": [
    "### PR Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43290e25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = get_ax(width=6, height=6)\n",
    "\n",
    "ax, df = add_precision_recall_curve_to_ax(df_training, y_col=label, prob_col='model_b_output_prob', model_name='Model B', ax=ax)\n",
    "\n",
    "add_precision_recall_annotation(ax, df, 0.5)\n",
    "add_precision_recall_annotation(ax, df, 0.25)\n",
    "add_precision_recall_annotation(ax, df, 0.02)\n",
    "\n",
    "force_ax_grid(ax, y_seperators=0.1, x_seperators=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6c5e2",
   "metadata": {},
   "source": [
    "## Distribution of Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b577dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_full, ax_fine  = get_ax(ncols=2, width=12)\n",
    "\n",
    "bins_fine = np.linspace(0, 0.1, 101)\n",
    "\n",
    "sns.histplot(\n",
    "    df_training.loc[lambda df: df[label] == False],\n",
    "    x='model_b_output_prob',\n",
    "    ax=ax_fine,\n",
    "    bins=bins_fine,\n",
    "    stat='percent',\n",
    "    cumulative=True,\n",
    "    label='False Matches',\n",
    "    fill=False,\n",
    "    element='step',\n",
    "    color=sns_colours[0],\n",
    ")\n",
    "\n",
    "sns.histplot(\n",
    "    df_training.loc[lambda df: df[label] == True],\n",
    "    x='model_b_output_prob',\n",
    "    ax=ax_fine,\n",
    "    bins=bins_fine,\n",
    "    stat='percent',\n",
    "    cumulative=True,\n",
    "    label='True Matches',\n",
    "    fill=False,\n",
    "    element='step',\n",
    "    color=sns_colours[1],\n",
    ")\n",
    "\n",
    "force_ax_grid(ax_fine, x_seperators=0.01, y_seperators=10)\n",
    "ax_fine.set(xlim=[0, 0.05], ylim=[0, 105], title='Distribution of P < 10%', xlabel='Model Probability')\n",
    "ax_fine.legend()\n",
    "\n",
    "\n",
    "bins_full = np.linspace(0, 1, 501)\n",
    "\n",
    "sns.histplot(\n",
    "    df_training.loc[lambda df: df[label] == False],\n",
    "    x='model_b_output_prob',\n",
    "    ax=ax_full,\n",
    "    bins=bins_full,\n",
    "    stat='percent',\n",
    "    cumulative=True,\n",
    "    label='False Matches',\n",
    "    fill=False,\n",
    "    element='step',\n",
    "    color=sns_colours[0],\n",
    ")\n",
    "\n",
    "\n",
    "sns.histplot(\n",
    "    df_training.loc[lambda df: df[label] == True],\n",
    "    x='model_b_output_prob',\n",
    "    ax=ax_full,\n",
    "    bins=bins_full,\n",
    "    stat='percent',\n",
    "    cumulative=True,\n",
    "    label='True Matches',\n",
    "    fill=False,\n",
    "    element='step',\n",
    "    color=sns_colours[1],\n",
    ")\n",
    "\n",
    "\n",
    "force_ax_grid(ax_full, x_seperators=0.1, y_seperators=10)\n",
    "ax_full.set(xlim=[0, 1], ylim=[0, 105], title='Distribution of Full Range', xlabel='Model Probability')\n",
    "ax_full.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d691b",
   "metadata": {},
   "source": [
    "## Highest Prob Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_full(\n",
    "    df_training\n",
    "    .sort_values('model_b_output_prob', ascending=False)\n",
    "    .loc[lambda df: df['model_b_output_prob'] > 0.9]\n",
    "    .T\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9c14c",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_cols = [\n",
    "    'standard_code',\n",
    "    'ref_no',\n",
    "    'version',\n",
    "    'standard_title',\n",
    "    'soc_2020_ext_code',\n",
    "    'soc_2020_ext_title',\n",
    "    'match_rank',\n",
    "    'soc_job_matches_standard_title',\n",
    "    'score_soc_job_match_standard_title',\n",
    "    'standard_typical_job_title',\n",
    "    'soc_job_matches_typical_job',\n",
    "    'score_soc_job_match_typical_job',\n",
    "    'score_overview',\n",
    "    'standard_overview',\n",
    "    'soc_2020_ext_description',\n",
    "    'soc_2020_matches_previous_assignment',\n",
    "    'soc_2020_major_group',\n",
    "    'soc_major_group_lower',\n",
    "    'soc_major_group_upper',\n",
    "    'level',\n",
    "    'soc_in_suggested_major_group',\n",
    "    'soc_2020_ext_is_nec',\n",
    "    'status',\n",
    "    'route',\n",
    "    'is_core_and_options',\n",
    "    'option_titles',\n",
    "    'ids_soc_2020_code',\n",
    "    'ids_soc_2020_rationale',\n",
    "    'soc_2020_code',\n",
    "    'soc_2020_title',\n",
    "    'soc_2010_code',\n",
    "    'autoassign_is_top',\n",
    "    'autoassign_in_top_2',\n",
    "    'autoassign_in_top_3',\n",
    "    'autoassign_in_top_5',\n",
    "    'autoassign_is_ranked',\n",
    "    'st_code_has_match_rank',\n",
    "    'score_rank',\n",
    "    'model_b_output_prob',\n",
    "    'rank_model_b',\n",
    "]\n",
    "\n",
    "export_cols = [c for c in export_cols if c in df_training.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68138d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_no_labels['model_b_output_prob'] = get_probabilites(\n",
    "    model_b,\n",
    "    df_training_no_labels[list_model_feature_cols_model_b]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688676a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output_modelling = (\n",
    "    pd.concat([\n",
    "        df_training.assign(st_code_has_match_rank=1),\n",
    "        df_training_no_labels.assign(st_code_has_match_rank=0),\n",
    "    ])      \n",
    "    .sort_values(['ref_no', 'score_rank'])\n",
    "    [export_cols] \n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "path_predicion_export= Path('nsfg_data', f'df_output_modelling_{run_version}.csv')\n",
    "df_output_modelling.to_csv(path_predicion_export, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24eaa50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5572c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Laurie's Analytical Kernel",
   "language": "python",
   "name": "venv_ana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
